# Tiny_GPT
 This is a very basic LLM project trained on tiny shakeshphere

 *Goal* - Create a Language Model based on transformers which autocompletes the given input upto s gien token length.

 *Inputs* - A sentence , no. of tokens user wants to be autocompleted

*Outputs* - Autocompleted text

*Training Data* - Little Shakesphere

*Required* - Knowledge of Pytorch, tranformers , optimizers(using AdamW here), Read the 'Attention is all you need' paper by google.

*Assumptions* - Tokens = chars

*Process*
